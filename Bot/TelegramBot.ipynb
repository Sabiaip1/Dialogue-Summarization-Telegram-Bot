{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "50xtxAlFlZ9p"
      },
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ aiogram –≤–µ—Ä—Å–∏–∏ 2.23.1\n",
        "!pip install --force-reinstall -v \"aiogram==2.23.1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "import logging\n",
        "import aiogram\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.utils import executor\n",
        "\n",
        "from aiogram.types import Message\n",
        "from aiogram.utils.markdown import hbold\n",
        "\n",
        "from aiogram.types import ReplyKeyboardMarkup, KeyboardButton, InlineKeyboardMarkup, InlineKeyboardButton\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtS49lDKlbOY",
        "outputId": "54f2f2cf-ab8c-4bdc-e119-16fd20fc5da3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "model_bart = AutoModelForSeq2SeqLM.from_pretrained(\"/content/gdrive/MyDrive/bartbase\")\n",
        "tokenizer_bart = AutoTokenizer.from_pretrained(\"/content/gdrive/MyDrive/bartbase\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zh906dolhhH",
        "outputId": "367e4aa1-6c0c-412d-9eab-140529d1a429"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏\n",
        "#uploaded_model = files.upload()\n",
        "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É\n",
        "#model_zip_path = next(iter(uploaded_model))\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "#uploaded_tokenizer = files.upload()\n",
        "# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É\n",
        "#tokenizer_zip_path = next(iter(uploaded_tokenizer))\n",
        "\n",
        "# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ zip-—Ñ–∞–π–ª–∞ —Å –º–æ–¥–µ–ª—å—é\n",
        "#with zipfile.ZipFile(model_zip_path, 'r') as model_zip:\n",
        "    #model_zip.extractall(\"/content/model/\")\n",
        "\n",
        "# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ zip-—Ñ–∞–π–ª–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º\n",
        "#with zipfile.ZipFile(tokenizer_zip_path, 'r') as tokenizer_zip:\n",
        "    #tokenizer_zip.extractall(\"/content/tokenizer/\")\n",
        "\n",
        "# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\n",
        "#model_path = \"/content/model/my_model\"\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "# –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É\n",
        "#tokenizer_path = \"/content/tokenizer/my_tokenizer\"\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "#tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
      ],
      "metadata": {
        "id": "5dH8XFIulmg3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞\n",
        "TOKEN = '–¢–û–ö–ï–ù'\n",
        "bot = Bot(token=TOKEN)\n",
        "dp = Dispatcher(bot)"
      ],
      "metadata": {
        "id": "SM3_nDy5lmqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "dialogue_history = {}\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞\n",
        "def trim_history(history, max_length=4096):\n",
        "    current_length = sum(len(message) for message in history.values())\n",
        "    while history and current_length > max_length:\n",
        "        removed_message = history.popitem(last=False)\n",
        "        current_length -= len(removed_message[1])\n",
        "    return history"
      ],
      "metadata": {
        "id": "7i30GkWclmwc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start\n",
        "@dp.message_handler(commands=['start'])\n",
        "async def start_command(message: types.Message):\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –∫–Ω–æ–ø–∫–∞–º–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞\n",
        "    markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
        "    btn1 = types.KeyboardButton(\"üá∑üá∫ –†—É—Å—Å–∫–∏–π\")\n",
        "    btn2 = types.KeyboardButton(\"üá¨üáß English\")\n",
        "    markup.add(btn1, btn2)\n",
        "    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π\n",
        "    await message.answer(\"–ü—Ä–∏–≤–µ—Ç!üëã –Ø —É–º–µ—é –∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏.\\n\\n –í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫\", reply_markup=markup)\n",
        "\n",
        "@dp.message_handler(lambda message: message.text in [\"üá∑üá∫ –†—É—Å—Å–∫–∏–π\", \"üá¨üáß English\"])\n",
        "async def language_selected(message: types.Message):\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫\n",
        "    selected_language = message.text.split(\" \")[1]\n",
        "    # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞\n",
        "    if selected_language == \"–†—É—Å—Å–∫–∏–π\":\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
        "        await message.answer(\"–Ø–∑—ã–∫ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ üá∑üá∫\")\n",
        "    elif selected_language == \"English\":\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
        "        await message.answer(\"Language changed to üá¨üáß\")"
      ],
      "metadata": {
        "id": "gAmOyGCnlm2H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help\n",
        "@dp.message_handler(commands=['help'])\n",
        "async def help_command(message: types.Message):\n",
        "    # –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥\n",
        "    help_text = \"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\\n\\n\"\n",
        "    help_text += \"/model - –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ \\n\"\n",
        "    help_text += \"/checkmodel - –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\\n\"\n",
        "    help_text += \"/clear - –æ—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏\\n\"\n",
        "    help_text += \"/help - –≤—ã–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥\\n\"\n",
        "    await message.answer(help_text)"
      ],
      "metadata": {
        "id": "y8bnfBnxmOD4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏\n",
        "current_model = model_bart\n",
        "current_tokenizer = tokenizer_bart\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /model\n",
        "@dp.message_handler(commands=['model'])\n",
        "async def model_command(message: types.Message):\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π\n",
        "    markup = types.InlineKeyboardMarkup()\n",
        "    btn1 = types.InlineKeyboardButton(\"bart-base\", callback_data='model_bart')\n",
        "    btn2 = types.InlineKeyboardButton(\"flan-t5-base\", callback_data='model')\n",
        "    markup.add(btn1, btn2)\n",
        "    await message.answer(\"–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:\", reply_markup=markup)\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –∏–Ω–ª–∞–π–Ω-–∫–Ω–æ–ø–∫–∏\n",
        "@dp.callback_query_handler(lambda c: c.data in ['model_bart', 'model'])\n",
        "async def process_model_selection(callback_query: types.CallbackQuery):\n",
        "    global current_model, current_tokenizer\n",
        "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏\n",
        "    if callback_query.data == 'model_bart':\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å 1\n",
        "        current_model = model_bart\n",
        "        current_tokenizer = tokenizer_bart\n",
        "        await callback_query.answer(\"–ú–æ–¥–µ–ª—å BART –≤—ã–±—Ä–∞–Ω–∞\")\n",
        "    elif callback_query.data == 'model':\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å 2\n",
        "        current_model = model\n",
        "        current_tokenizer = tokenizer\n",
        "        await callback_query.answer(\"–ú–æ–¥–µ–ª—å T5 –≤—ã–±—Ä–∞–Ω–∞\")\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /checkmodel, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–≤–æ–¥–∏—Ç —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å\n",
        "@dp.message_handler(commands=['checkmodel'])\n",
        "async def checkmodel_command(message: types.Message):\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏\n",
        "    if current_model == model_bart:\n",
        "        current_model_name = \"bart-base\"\n",
        "    else:\n",
        "        current_model_name = \"flan-t5-base\"\n",
        "    await message.answer(f\"–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: {current_model_name}\")"
      ],
      "metadata": {
        "id": "bURc0YzLmOKF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /clear\n",
        "@dp.message_handler(commands=['clear'])\n",
        "async def process_clear_command(message: types.Message):\n",
        "    user_id = message.from_user.id\n",
        "    dialogue_history[user_id] = []\n",
        "    await message.reply(\"–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞.\")"
      ],
      "metadata": {
        "id": "8ToRZ4fHmOOg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é\n",
        "@dp.message_handler()\n",
        "async def summarize_handler(message: types.Message) -> None:\n",
        "    \"\"\"\n",
        "    Handler that summarizes the received message using a pre-trained model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_input = message.text\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤\n",
        "        if not all(ord(char) < 128 for char in user_input):\n",
        "            await message.answer(\"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –í–∞—Å –Ω–µ –ø–æ–Ω–∏–º–∞—é.\")\n",
        "            return\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞\n",
        "        dialogue_history[len(dialogue_history) + 1] = user_input\n",
        "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑\n",
        "        input_ids = current_tokenizer.encode(user_input, return_tensors='pt')\n",
        "        output_ids = current_model.generate(input_ids, max_length=200, num_return_sequences=1, early_stopping=True)\n",
        "        summary = current_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–≤–æ–¥–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π\n",
        "        if summary:\n",
        "            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Å–∫–∞–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é\n",
        "            await message.answer(f\"Summary: {summary}\")\n",
        "        else:\n",
        "            raise ValueError(\"–°–≤–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –±—ã–ª–∞ –ø–æ–ª—É—á–µ–Ω–∞.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"–û—à–∏–±–∫–∞ –≤ summarize_handler: {e}\")\n",
        "        await message.answer(\"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.\")\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞\n",
        "def main() -> None:\n",
        "    executor.start_polling(dp, skip_updates=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB1Z2lmBmb6u",
        "outputId": "272f06fe-412d-404c-f39a-cea52b8bcb3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram:Updates were skipped successfully.\n",
            "WARNING:aiogram:Goodbye!\n"
          ]
        }
      ]
    }
  ]
}